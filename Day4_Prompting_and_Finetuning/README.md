# ğŸ—£ï¸ Day 4: Prompting + Finetuning

## ğŸ§  What I Learned
- Zero-shot vs Few-shot vs Chain-of-Thought prompting
- Pretraining vs Finetuning vs Instruction Tuning
- Model families: GPT, BERT, T5, LLaMA

## ğŸ”¬ Experimented With
- Prompt variations using GPT2 (Hugging Face)
- Tasks: summarization, translation, reasoning

## ğŸ”§ Tools Used
- Hugging Face Transformers
- Google Colab

- ## ğŸ”— Notebook Link
[ğŸ”— Open Day 4 Notebook in Colab](https://colab.research.google.com/drive/1Np3l4-sU-QfUEiSaL5d-Q1L1eMjRauUU)
