# âš¡ Day 2: Transformers & Attention

## ðŸ§  What I Learned
- Self-attention and multi-head attention
- Positional encoding and parallelism
- Transformer architecture

## ðŸ§ª Experiment
- Visualized attention using HuggingFace

## ðŸ”§ Tools Used
- HuggingFace Transformers
- TensorBoard (optional)

- - ## ðŸ”— Notebook Link
[ðŸ”— Open Day 1 Notebook in Colab](https://colab.research.google.com/drive/18xGsltoVVteByCZtPiAXXesXYy2sfvfr)
