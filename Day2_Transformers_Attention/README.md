# âš¡ Day 2: Transformers & Attention

## ðŸ§  What I Learned
- Self-attention and multi-head attention
- Positional encoding and parallelism
- Transformer architecture

## ðŸ§ª Experiment
- Visualized attention using HuggingFace

## ðŸ”§ Tools Used
- HuggingFace Transformers
- TensorBoard (optional)